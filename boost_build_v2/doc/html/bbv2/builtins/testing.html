<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<title>Testing</title>
<link rel="stylesheet" href="../../boostbook.css" type="text/css">
<meta name="generator" content="DocBook XSL Stylesheets V1.69.1">
<link rel="start" href="../../index.html" title="Boost.Build V2 User Manual">
<link rel="up" href="../tasks.html" title="Chapter 5. Common tasks">
<link rel="prev" href="../tasks/installing.html" title="Installing">
<link rel="next" href="raw.html" title="Custom commands">
</head>
<body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF">
<table cellpadding="2" width="100%"><tr><td valign="top"><img alt="Boost C++ Libraries" width="277" height="86" src="../../../../../../../boost.png"></td></tr></table>
<hr>
<div class="spirit-nav">
<a accesskey="p" href="../tasks/installing.html"><img src="../../../../../../../doc/html/images/prev.png" alt="Prev"></a><a accesskey="u" href="../tasks.html"><img src="../../../../../../../doc/html/images/up.png" alt="Up"></a><a accesskey="h" href="../../index.html"><img src="../../../../../../../doc/html/images/home.png" alt="Home"></a><a accesskey="n" href="raw.html"><img src="../../../../../../../doc/html/images/next.png" alt="Next"></a>
</div>
<div class="section" lang="en">
<div class="titlepage"><div><div><h2 class="title" style="clear: both">
<a name="bbv2.builtins.testing"></a>Testing</h2></div></div></div>
<p>
      Boost.Build has convenient support for running unit tests. The simplest
      way is the <code class="computeroutput">unit-test</code> rule, which follows the <a href="../overview/targets.html#bbv2.main-target-rule-syntax">common syntax</a>. For example:
</p>
<pre class="programlisting">
unit-test helpers_test : helpers_test.cpp helpers ;
</pre>
<p>
    </p>
<p>
      The <code class="computeroutput">unit-test</code> rule behaves like the
      <a href="../tasks.html#bbv2.tasks.programs" title="Programs">exe</a> rule, but after the executable is created
      it is also run. If the executable returns an error code, the build system
      will also return an error and will try running the executable on the next
      invocation until it runs successfully. This behaviour ensures that you can
      not miss a unit test failure.
    </p>
<p>
      By default, the executable is run directly. Sometimes, it is desirable to
      run the executable using some helper command. You should use the <code class="literal">
      testing.launcher</code> property to specify the name of the helper
      command. For example, if you write:
</p>
<pre class="programlisting">
unit-test helpers_test
   : helpers_test.cpp helpers
   : <span class="bold"><strong>&lt;testing.launcher&gt;valgrind</strong></span>
   ;
</pre>
<p>
      The command used to run the executable will be:
</p>
<pre class="screen">
<span class="bold"><strong>valgrind</strong></span> bin/$toolset/debug/helpers_test
</pre>
<p>
    </p>
<p>
       There are few specialized testing rules, listed below:
</p>
<pre class="programlisting">
rule compile ( sources : requirements * : target-name ? )
rule compile-fail ( sources : requirements * : target-name ? )
rule link ( sources + : requirements * : target-name ? )
rule link-fail ( sources + : requirements * : target-name ? )
</pre>
<p>
      They are given a list of sources and requirements. If the target name is
      not provided, the name of the first source file is used instead. The
      <code class="literal">compile*</code> tests try to compile the passed source. The
      <code class="literal">link*</code> rules try to compile and link an application from
      all the passed sources. The <code class="literal">compile</code> and <code class="literal">link
      </code> rules expect that compilation/linking succeeds. The <code class="literal">
      compile-fail</code> and <code class="literal">link-fail</code> rules expect that
      the compilation/linking fails.
    </p>
<p>
      There are two specialized rules for running applications, which are more
      powerful than the <code class="computeroutput">unit-test</code> rule. The <code class="computeroutput">run</code> rule
      has the following signature:
</p>
<pre class="programlisting">
rule run ( sources + : args * : input-files * : requirements * : target-name ?
    : default-build * )
</pre>
<p>
      The rule builds application from the provided sources and runs it, passing
      <code class="varname">args</code> and <code class="varname">input-files</code> as command-line
      arguments. The <code class="varname">args</code> parameter is passed verbatim and
      the values of the <code class="varname">input-files</code> parameter are treated as
      paths relative to containing Jamfile, and are adjusted if <span><strong class="command">bjam
      </strong></span> is invoked from a different directory. The
      <code class="computeroutput">run-fail</code> rule is identical to the <code class="computeroutput">run</code> rule,
      except that it expects that the run fails.
    </p>
<p>
      All rules described in this section, if executed successfully, create a
      special manifest file to indicate that the test passed. For the
      <code class="computeroutput">unit-test</code> rule the files is named <code class="filename"><em class="replaceable"><code>
      target-name</code></em>.passed</code> and for the other rules it is
      called <code class="filename"><em class="replaceable"><code>target-name</code></em>.test</code>.
      The <code class="computeroutput">run*</code> rules also capture all output from the program, and
      store it in a file named <code class="filename"><em class="replaceable"><code>
      target-name</code></em>.output</code>.
    </p>
<p>
      <a class="indexterm" name="idp3269360"></a>
      If the <code class="literal">preserve-test-targets</code> feature has the value
      <code class="literal">off</code>, then <code class="computeroutput">run</code> and the <code class="computeroutput">run-fail</code>
      rules will remove the executable after running it.  This somewhat decreases
      disk space requirements for continuous testing environments. The default 
      value of <code class="literal">preserve-test-targets</code> feature is <code class="literal">on</code>.      
    </p>
<p>
      It is possible to print the list of all test targets (except for
      <code class="computeroutput">unit-test</code>) declared in your project, by passing the <code class="literal">
      --dump-tests</code> command-line option. The output will consist of
      lines of the form:
</p>
<pre class="screen">
boost-test(<em class="replaceable"><code>test-type</code></em>) <em class="replaceable"><code>path</code></em> : <em class="replaceable"><code>sources</code></em>
</pre>
<p>
    </p>
<p>
      It is possible to process the list of tests, Boost.Build output
      and the presense/absense of the <code class="filename">*.test</code>
      files created when test passes into human-readable status table of tests.
      Such processing utilities are not included in Boost.Build.
    </p>
</div>
<table xmlns:rev="http://www.cs.rpi.edu/~gregod/boost/tools/doc/revision" width="100%"><tr>
<td align="left"></td>
<td align="right"><div class="copyright-footer">Copyright © 2006-2009 Vladimir Prus<p>Distributed under the Boost Software License, Version 1.0.
      (See accompanying file <code class="filename">LICENSE_1_0.txt</code> or copy at 
      <a href="http://www.boost.org/LICENSE_1_0.txt" target="_top">http://www.boost.org/LICENSE_1_0.txt</a>)
      </p>
</div></td>
</tr></table>
<hr>
<div class="spirit-nav">
<a accesskey="p" href="../tasks/installing.html"><img src="../../../../../../../doc/html/images/prev.png" alt="Prev"></a><a accesskey="u" href="../tasks.html"><img src="../../../../../../../doc/html/images/up.png" alt="Up"></a><a accesskey="h" href="../../index.html"><img src="../../../../../../../doc/html/images/home.png" alt="Home"></a><a accesskey="n" href="raw.html"><img src="../../../../../../../doc/html/images/next.png" alt="Next"></a>
</div>
</body>
</html>
